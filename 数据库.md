##  数据库系统原理

### 1. 基础知识

#### 相关概念

**四个基本概念**

- **数据（Data）**：数据描述事物的符号记录；常见的种类有：文本，图形，图像，音频，视频等；特点是：数据与其语义是不可分的。
- **数据库（Database）**：是长期存储在计算机内、有组织的、可共享的大量数据的集合。基本特征：可共享，冗余度较小，数据独立性较高，易扩展，数据按照一定的模型组织、描述和存储。
- **数据库管理系统（DBMS）**：位于用户和操作系统之间的一层数据管理软件。主要功能：数据定义功能，数据组织、存储和管理，数据操纵功能，数据库的事物管理和运行管理，数据库的建立和维护功能。
- **数据库系统（Database System）**：是由数据库，数据库管理系统，应用系统和数据库管理人员组成的存储、管理、处理和维护数据的系统。

**数据库系统的特点：**数据结构化，数据共享性高、冗余度低、易扩充，数据独立性高，数据由DBMS统一管理和控制。

**关于码的概念**：

- **候选码**：若关系中的某一属性组的值能唯一地标识一个元组， 则称该属性组为候选码  
- **全码**：关系模式的所有属性组是这个关系模式的候选码， 称为全码  
- **主码**：若一个关系有多个候选码， 则选定其中一个为主码  
- **外码**：设F是基本关系R的一个或一组属性， F不是关系R的码，但F是另一个关系的码， 则称F是基本关系R的外码  
- 候选码的每个属性都是主属性，不包含在任何侯选码中的属性称为非主属性或非码属性

**基本关系的性质**：

1. 列是同质的，即每一列中的分量是同类型数据，来自同一个域
2. 不同的列可能来自同一个域
3. 列的顺序无所谓，列的次序可以任意交换
4. 行的顺序无所谓，行的次序可以任意交换
5. 任意连个元组的候选码不能相同
6. 分量必须是原子值

**关系的完整性**：

- 实体完整性：若属性A是关系R的主属性，则A不能取空值
- 参照完整性：若属性（或属性组）F是基本关系R的外码，它与基本关系S的主码Ks相对应（基本关系R和S不一定是不同的关系），则对于R中每个元组在F上的值必须为空值或者等于S中某个元组的主码值    
- 用户定义完整性：针对某一具体关系数据库的约束条件， 反映某一具体应用所涉及的数据必须满足的语义要求  

#### 数据模型

常用数据模型：非关系模型（包括层次模型、网状模型），关系模型，面向对象模型，对象关系模型。

**层次模型**

层次模型需要满足以下两个基本条件：

1. 有且只有一个节点没有双亲节点，这个节点称为根节点。
2. 根以外的节点有且只有一个双亲节点。

**网状模型**

网状模型需要满足以下两个基本条件：

1. 允许一个以上的节点无双亲
2. 一个节点可以有多于一个的双亲

**关系模型**

相关概念：

> 关系：一个关系对应一张表
>
> 元组：表中的一行即为一个元组
>
> 属性：表中的一列即为一个属性
>
> 主码：表中的一个属性组，可以唯一确定一个元组
>
> 域：属性的取值范围
>
> 分量：元组中的一个属性值

关系的完整性约束条件：实体完整性，参照完整性，用户定义的完整性

### 2. 关系数据理论

#### 2.1 数据依赖

1. **函数依赖**

    记 A->B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。

    如果 {A1，A2，... ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。

    对于 A->B，如果能找到 A 的真子集 A'，使得 A'-> B，那么 A->B 就是**部分函数依赖**，否则就是完全函数依赖。

    对于 A->B，B->C，则 A->C 是一个**传递函数依赖**。

2. **多值依赖**：

**数据依赖带来的问题**：

| Sno  | Sname  | Sdept  | Mname  | Cname  | Grade |
| :--: | :----: | :----: | :----: | :----: | :---: |
|  1   | 学生-1 | 学院-1 | 院长-1 | 课程-1 |  90   |
|  2   | 学生-2 | 学院-2 | 院长-2 | 课程-2 |  80   |
|  2   | 学生-2 | 学院-2 | 院长-2 | 课程-1 |  100  |
|  3   | 学生-3 | 学院-2 | 院长-2 | 课程-2 |  95   |

- 数据冗余：如上表院长-2重复出现
- 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改
- 插入异常：例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入
- 删除异常：删除一个信息，那么也会丢失其它信息。例如删除了课程-1需要删除第一行和第三行，那么学生-1的信息就会丢失。

#### 2.2 范式

##### 1NF

如果关系模式R的**所有属性都是不可分的基本数据项**，则R∈ 1NF  

##### 2NF

若R∈ 1NF， 且**每一个非主属性完全函数依赖于码**， 则R∈ 2NF 

可以采用投影分解法将一个1NF的关系分解为多个2NF的关系，从一定程度上减轻1NF关系中存在的问题，但不能完全消除这些问题。

##### 3NF

若R∈ 2NF，且**每一个非主属性不传递依赖于码**(这使得非主属性之间不存在函数依赖)，则R∈ 3NF。

##### BCNF

若关系R的**每一个决定因素都包含码**，那么R∈ BCNF。

**并不是规范化程度越高， 模式就越好**，必须结合应用环境和现实世界的具体情况合理地选择数据库模式。  

### 3. 事务

#### 3.1 概念

事务是逻辑上的一组操作，要么都执行，要么都不执行。

**典型的MySQL事务是如下操作**：

```mysql
start transaction;
……  #一条或多条sql语句
commit;
```

​	其中 start transaction 标识事务开始，commit 提交事务，将执行结果写入到数据库。如果 sql 语句执行出现问题，会调用rollback，回滚所有已经执行成功的 sql 语句。

​	MySQL中默认采用的是自动提交（autocommit）模式，**如果没有 start transaction 显式地开始一个事务，那么每个 sql 语句都会被当做一个事务执行提交操作**。可以通过 `set autocommit = 0;` 来关闭自动提交，但是需要注意的是 **autocommit 参数是针对连接的，在一个连接中修改了参数，不会对其他连接产生影响**。

#### 3.2 事务的特点(ACID)

1. **原子性（Atomicity）**：事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。
2. **⼀致性（Consistency）**：事务执行结束后，**数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态**。
3. **隔离性（Isolation）**：一个事务的内部操作和使用的数据对其他并发的事务是隔离的，并发执行的各个事务之间不能相互干扰。
4. **持久性（Durability）**：一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。

事务的**隔离性是通过锁实现，而事务的原子性、一致性和持久性则是通过事务日志实现**。

#### 3.3 事务日志

参考链接[redo/undo log、binlog 的详解及其区别](https://www.jianshu.com/p/57c510f4ec28)

- **redo log（重做日志）** 实现持久性

    > ​	InnoDB 作为 MySQL 的存储引擎，数据是存放在磁盘中的，但如果每次读写数据都需要磁盘 IO，效率会很低。为此，InnoDB 提供了缓存（`Buffer Pool`），`Buffer Pool`中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：当从数据库读取数据时，会首先从`Buffer Pool`中读取，如果`Buffer Pool`中没有，则从磁盘读取后放入`Buffer Pool`；当向数据库写入数据时，会首先写入`Buffer Pool`，`Buffer Pool`中修改的数据会定期刷新到磁盘中，这一过程称为“刷脏”。`Buffer Pool`的使用大大提高了读写数据的效率，但是也带了新的问题：如果 MySQL 宕机，而此时`Buffer Pool`中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。

    ​	`redo log` 包含两部分：一是内存中的日志缓冲，二是磁盘上的重做日志文件(`redo log file`)。**事务开启时**，事务中的操作，都会**先写入存储引擎的日志缓冲中**，在**事务提交之前**，这些**缓冲的日志都需要提前刷新到磁盘上持久化**，当事务提交之后，在`Buffer Pool`中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统重启进行恢复时，就可以根据`redo log`中记录的日志，把数据库恢复到崩溃前的一个状态。未完成的事务，可以继续提交，也可以选择回滚，这基于恢复的策略而定。

    ​	`redo log`是以顺序追加的方式记录的，所有的事务共享`redo log`的存储空间，它们的`redo log`按语句的执行顺序，依次交替的记录在一起。

- **undo log（回滚日志）** 实现原子性

    回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。

​    `redo log`是恢复提交事务修改的页操作，而`undo log`是回滚行记录到特定版本。二者记录的内容也不同，`redo log`是**物理日志**，记录的是 **数据页** 的物理修改，而不是某一行或某几行修改成怎样怎样，而`undo log`是**逻辑日志**，根据每行记录进行记录。

- **binlog**
    - statement：基于SQL语句的模式，某些语句中含有一些函数，例如 `UUID` `NOW` 等在复制过程可能导致数据不一致甚至出错。
    - row：基于行的模式，记录的是行的变化，很安全。但是 binlog 的磁盘占用会比其他两种模式大很多，在一些大表中清除大量数据时在 binlog 中会生成很多条语句，可能导致从库延迟变大。
    - mixed：混合模式，根据语句来选用是 statement 还是 row 模式

**其他类型日志**：

- **错误日志**：记录出错信息，也记录一些警告信息或者正确的信息。
- **查询日志**：记录所有对数据库请求的信息，不论这些请求是否得到了正确的执行。
- **慢查询日志**：设置一个阈值，将运行时间超过该值的所有SQL语句都记录到慢查询的日志文件中。
- **二进制日志**：记录对数据库执行更改的所有操作。

#### 3.4 MySQL的分布式事务

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/MySQL%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%A8%A1%E5%9E%8B.png" style="zoom:70%;" />

如图，MySQL 的分布式事务模型。模型中分三块：应用程序（AP）、资源管理器（RM）、事务管理器（TM）:

- 应用程序：定义了事务的边界，指定需要做哪些事务；
- 资源管理器：提供了访问事务的方法，通常一个数据库就是一个资源管理器；
- 事务管理器：协调参与了全局事务中的各个事务。

分布式事务采用两段式提交（`two-phase commit`）的方式：

- 第一阶段所有的事务节点开始准备，告诉事务管理器`ready`。
- 第二阶段事务管理器告诉每个节点是`commit`还是`rollback`。如果有一个节点失败，就需要全局的节点全部`rollback`，以此保障事务的原子性。

#### 3.5 并发事务存在的问题

- **丢失修改**：在⼀个事务读取⼀个数据时，另外⼀个事务也访问了该数据，那么在第⼀个事务中修改了这个数据后，第⼆个事务也修改了这个数据。这样第⼀个事务内的修改结果就被丢失，因此称为丢失修改。  
- **脏读**：事务A修改了数据但未提交，这时事务B读取了该数据，然后事务A回滚操作，那么事务B读取到的数据是脏数据。
- **不可重复读**：事务 A 多次读取同一数据，事务B在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。
- **幻读**：一个事务A读取了某个范围的数据，接着另一个并发事务B在这个范围内插入了一些数据，在随后的查询中，事务A就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

产生并发不一致性问题的主要原因是破坏了事务的隔离性，**解决方法是通过并发控制来保证隔离性**。

#### 3.6 事务隔离级别

  <div>
    <table style="text-align: center;width:650px">
        <caption style="font-weight:800;">隔离级别能解决的并发一致性问题</caption>
        <tr>
            <th>隔离级别</th>
            <th>脏读</th>
            <th>不可重复读</th>
            <th>幻读</th>
        </tr>
        <tr>
            <td>读未提交</td>
            <td>×</td>
            <td>×</td>
            <td>×</td>
        </tr>
        <tr>
            <td>读已提交</td>
            <td>√</td>
            <td>×</td>
            <td>×</td>
        </tr>
        <tr>
            <td>可重复读</td>
            <td>√</td>
            <td>√</td>
            <td>×</td>
        </tr>
        <tr>
            <td>可串行化</td>
            <td>√</td>
            <td>√</td>
            <td>√</td>
        </tr>
    </table>
</div>

- **READ-UNCOMMITTED（读未提交）**：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。
- **READ-COMMITTED（读已提交）**：允许读取并发事务已经提交的数据，可以阻⽌脏读，但是幻读或不可重复读仍有可能发⽣。
- **REPEATABLE-READ（可重复读）**：对同⼀字段的多次读取结果都是⼀致的，除⾮数据是被本身事务⾃⼰所修改，可以阻⽌脏读和不可重复读，但幻读仍有可能发⽣。==**MySQL的默认事务隔离级别**==
- **SERIALIZABLE（可串行化）**：最⾼的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执⾏，这样事务之间就完全不可能产⽣⼲扰，也就是说，该级别可以防⽌脏读、不可重复读以及幻读。 

> InnoDB 存储引擎在 **REPEATABLE-READ**（可重复读）事务隔离级别下使用的是 [Next-Key Lock](#nextKeyLock) 算法，可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。

### 4. 并发控制

#### 4.1 锁

##### 锁的分类

###### 按对数据操作的类型分

- **排他锁（x锁，也称为写锁）**：当前写操作没有完成前，它会阻断其他写锁和读锁。**在更新操作(INSERT、UPDATE 或 DELETE)过程中始终应用排它锁**。
- **共享锁（s锁，也称为读锁）**：针对同一份数据，多个读操作可以同时进行，不会互相影响。

一个事务对数据对象 A 加了 X 锁，就可以对 A 进行读取和更新。加锁期间其它事务不能对 A 加任何锁。一个事务对数据对象 A 加了 S 锁，可以对 A 进行读取操作，但是不能进行更新操作。加锁期间其它事务能对 A 加 S 锁，但是不能加 X 锁。

​	**上共享锁的写法**：select ... lock in share mode

​		例如： select  * from table where id=1 lock in share mode；

​	**上排它锁的写法**：select ... for update

​		例如：select * from table where id=1 for update；

###### 按对数据操作的粒度分

- **表级锁**：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低（MyISAM 和 MEMORY 存储引擎采用的是表级锁）。
- **行级锁**：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高（InnoDB 存储引擎既支持行级锁也支持表级锁，但默认情况下是采用行级锁）。
- **页面锁**：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。

##### MyISAM的锁

MyISAM 的表锁有两种模式：

- 表共享读锁 （Table Read Lock）：不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求。
- 表独占写锁 （Table Write Lock）：会阻塞其他用户对同一表的读和写操作。

默认情况下，写锁比读锁具有更高的优先级：当一个锁释放时，这个锁会优先给写锁队列中等候的获取锁请求。

##### InnoDB的锁

InnoDB的行锁同样也分为共享锁和排他锁。

**注意：行锁必须有索引才能实现，否则会自动锁全表，那么就不是行锁了**。

```sql
# 明确指定主键，并且有此记录，row lock
SELECT * FROM tablename WHERE id='3' FOR UPDATE;

# 明确指定主键，若查无此记录，无lock
SELECT * FROM tablename WHERE id='-1' FOR UPDATE;

# 无主键，table lock
SELECT * FROM tablename WHERE name='jojo' FOR UPDATE;

# 主键不明确，table lock
SELECT * FROM tablename WHERE id<>'3' FOR UPDATE;
```

​    用索引字段做为条件进行修改时， **是否表锁的取决于这个索引字段能否确定记录唯一**，当索引值对应记录不唯一，会进行锁表，相反则行锁。

###### InnoDB的三种行锁算法

1. **记录锁(Record Locks)**：单个行记录上的锁。对索引项加锁，而不是对记录本身加锁，锁定符合条件的行。其他事务不能修改和删除加锁项；如果表没有设置索引，InnoDB 会自动在主键上创建隐藏的聚簇索引，因此 Record Locks 依然可以使用。

    在通过 **主键索引** 与 **==唯一==索引** 对数据行进行 UPDATE 操作时，也会对该行数据加记录锁。

2. **间隙锁（Gap Locks）**：当使用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁。**对于键值在条件范围内但并不存在的记录，叫做“间隙”，InnoDB 也会对这个“间隙”加锁**，这种锁机制就是所谓的间隙锁。只会锁定索引之间的间隙，但是不包含索引本身。

    例如：

    ```sql
    SELECT * FROM table WHERE id BETWEN 1 AND 10 FOR UPDATE;
    ```

    所有在`（1，10）`区间内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。

    GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。

3. <span id="nextKeyLock">**临键锁(Next-key Locks)**</span>：**临键锁**，是**记录锁与间隙锁的组合**，它的封锁范围，既包含索引记录，又包含索引区间。它锁定一个前开后闭区间，例如一个索引包含以下值：10, 11, 13, 20，那么就需要锁定以下区间：

    ```javascript
    (-∞, 10]、(10, 11]、(11, 13]、(13, 20]、(20, +∞)
    ```

    当查询的索引含有唯一属性时，InnoDB存储引擎会对Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围；当查询的索引为辅助索引时，默认使用Next-Key Locking技术进行加锁，锁定范围是前一个索引到后一个索引之间范围。

InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是**表锁**：

- 意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。
- 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。

​    任意 IS/IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁。IX，IS是表级锁，不会和行级的X，S锁发生冲突，只会和表级的X，S发生冲突。

​    如果没有意向锁，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。

##### 封锁协议

- **一级封锁协议**：事务 T 要修改数据 A 时必须加 X 锁，**直到 T 结束才释放锁**（结束包括正常结束`commit`和非正常结束`rollback`）。可以解决丢失修改问题，但不能解决不可重复读和脏读的问题。
- **二级封锁协议**：在一级的基础上，要求事务 T 读取数据 A 时必须加 S 锁，**读取完马上释放** S 锁。可以解决丢失修改和脏读（如果一个事务在对数据 A 进行修改，根据 1 级封锁协议，会加 X 锁，那么就不能再加 S 锁了，也就是不会读入数据）的问题，不能解决不可重复读的问题。
- **三级封锁协议**：在一级的基础上，要求读取数据 A 时必须加 S 锁，**直到事务结束**了才能释放 S 锁。可以解决丢失修改、脏读、不可重复读（事务 T 对数据 A 加了 S 锁，其他事务只能对 A 加 S 锁而不能加 X 锁，即其他事务只能读取 A 而不能修改它，只能等到事务 T 结束）的问题。

##### 死锁和活锁

**死锁**是指两个或多个事务都已经封锁了一些数据对象，然后又都请求锁定其他事务已封锁的数据对象，从而导致循环等待。

解决死锁的方法：

- 死锁的预防

    1. 一次性封锁法：要求事务一次性将所有数据全部加锁。存在问题：降低了系统的并发性、难以精确的确定所有的数据对象
    2. 顺序封锁法：对数据对象规定一个封锁顺序，所有事务都按照这个顺序封锁。存在问题：维护成本高、很难实现

- 死锁的诊断和解除

    诊断采用超时法或等待图法。

    解除：选择一个处理死锁代价最小的事务， 将其撤消释放此事务持有的所有的锁， 使其它事务能继续运行下去  

事务T1封锁了数据R，事务T2又请求封锁R，于是T2等待；T3也请求封锁R，当T1释放了R上的封锁之后系统首先批准了T3的请求，T2仍然等待；T4又请求封锁R，当T3释放了R上的封锁之后系统又批准了T4的请求……T2有可能永远等待， 这就是**活锁**的情形。

避免活锁的简单方法是采用先来先服务策略。  

##### 乐观锁和悲观锁

乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务。

悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁。

#### 4.2 MVCC 多版本并发控制

​	多版本并发控制（Multi-Version Concurrency Control, MVCC）是 MySQL 的 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现 读已提交 和 可重复读 这两种隔离级别，**MVCC也只在这两种隔离级别下工作**。

​	MVCC 的实现是通过保存数据在某个时间点的快照来实现的。也就是说不管需要执行多长时间，每个事物看到的数据都是一致的。

<!-- 分页-->

<div style="page-break-after: always;"></div>

<!--分页-->

## MySQL

### 1. MySQL 架构

- **连接层**：最上层是一些客户端和连接服务。**主要完成一些类似于连接处理、授权认证、及相关的安全方案**。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。
- **服务层**：第二层服务层，主要完成大部分的核心服务功能， 包括查询解析、分析、优化、缓存、以及所有的内置函数，所有跨存储引擎的功能也都在这一层实现，包括触发器、存储过程、视图等。
- **引擎层**：第三层存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。
- **存储层**：第四层为数据存储层，主要是将数据存储在运行于该设备的文件系统之上，并完成与存储引擎的交互。

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/MySQL%E6%9E%B6%E6%9E%84.webp" style="zoom:70%;" />

**一条 SQL 语句在 MySQL 中的执行流程**
客户端请求 $\dashrightarrow$ 连接器（验证用户身份，给予权限）$\dashrightarrow$ 查询缓存（存在缓存则直接返回，否则继续执行后续操作）$\dashrightarrow$ 分析器（对 SQL 进行语法分析和词法分析）$\dashrightarrow$ 优化器（对执行的 sql 优化，选择最优的执行方案方法）$\dashrightarrow$ 执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口）$\dashrightarrow$ 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果）

### 2. MySQL存储引擎

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/mysql%E5%BC%95%E6%93%8E.png" style="zoom:100%;" />

常见的存储引擎 InnoDB、MyISAM、Memory

#### MyISAM 和 InnoDB 对比

1. InnoDB 支持事务，MyISAM 不支持事务；
2. InnoDB 支持外键，MyISAM 不支持；
3. InnoDB 是聚簇索引，MyISAM 是非聚簇索引。**聚簇索引：**将数据存储与索引放到了一块，索引结构的叶子节点保存了行数据；**非聚簇索引：**将数据与索引分开存储，索引结构的叶子节点存储了数据对应的位置。
4. MyISAM 只支持表锁，而 InnoDB 还支持行锁；
5. InnoDB 不保存表的具体行数，执行`select count(*) from table` 时需要全表扫描。而 MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快。

### 3. MySQL索引

#### 分类

从数据结构角度分类：

1. [B+ 树索引](#b+树)
2. [Hash 索引](#hashIndex)
3. [全文索引（Full-Text）](#fullTextIndex)
4. [空间索引（R-Tree）](#RTreeIndex)

从逻辑角度分类：

- 主键索引：主键索引是一种特殊的唯一索引，不允许有空值。
- 普通索引或者单列索引：每个索引只包含单个列，一个表可以有多个单列索引。
- 多列索引（复合索引、联合索引）：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用复合索引时遵循最左前缀集合。
- 唯一索引或者非唯一索引。
- 空间索引：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。创建空间索引的列，必须将其声明为NOT NULL，空间索引只能在存储引擎为MYISAM的表中创建。

从物理存储角度分类：

​	聚簇索引和非聚簇索引

##### <span id="b+树">B+ 树索引</span>

是大多数 MySQL 存储引擎的默认索引类型。

因为不再需要进行全表扫描，只需要对树进行搜索即可， 所以查找速度快很多。

因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。

可以指定多个列作为索引列，多个索引列共同组成键。

适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。

**主键索引与辅助索引结构**

<div style="display:flex; text-align:center;">
    <div style="flex:1;">
        <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/InnoDB%E4%B8%BB%E9%94%AE%E7%B4%A2%E5%BC%95%E4%B8%8E%E8%BE%85%E5%8A%A9%E7%B4%A2%E5%BC%95.jpg" style="zoom:80%;" />
    </div>
    <div style="flex:1;">
        <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/MyISAM%E4%B8%BB%E9%94%AE%E7%B4%A2%E5%BC%95%E4%B8%8E%E8%BE%85%E5%8A%A9%E7%B4%A2%E5%BC%95.jpg" style="zoom:80%;" />
    </div>
</div>

- **InnoDB引擎索引结构的叶子节点的数据域，存放的就是实际的数据记录**，对于主索引，此处会存放表中所有的数据记录；对于辅助索引此处会引用主键，检索的时候通过主键到主键索引中找到对应数据行，因此通过辅助索引进行条件搜索需要两个步骤：(1)在辅助索引上检索相关条件，到达其叶子节点获取对应的主键；(2)使用主键在主索引上再进行对应的检索操作。这就是所谓的”回表查询“。

    **辅助索引的叶子节点存储的是主键值，是为了保证数据一致性和节省存储空间**。如果辅助索引的叶子节点也存放的是实际数据，那么当数据发生更改的时候，需要修改多次，而且数据存储了多次很浪费空间。

- MyISAM引擎的索引文件和数据文件是分离的。**MyISAM引擎索引结构的叶子节点的数据域，存放的并不是实际的数据记录，而是数据记录的地址**。MyISAM的主索引与辅助索引区别并不大，只是主键索引不能有重复的关键字。通过索引查找数据的流程：先从索引文件中查找到索引节点，从中拿到数据的文件指针，再到数据文件中通过文件指针定位了具体的数据。辅助索引类似。

##### <span id="hashIndex">hash索引</span>

哈希索引能以 O(1) 时间进行查找，但是失去了有序性：

- 无法用于排序与分组；
- 只支持精确查找，无法用于部分查找和范围查找。

InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。

##### <span id="fullTextIndex">全文索引</span>

MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。

查找条件使用 MATCH AGAINST，而不是普通的 WHERE。

全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。

InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。

##### <span id="RTreeIndex">空间索引</span>

MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。

#### 索引的优化

1. 独立的列

    在进行查询时，索引列不能是表达式的一部分，也不能是函数的参数，否则无法使用索引。

    例如下面的查询不能使用 actor_id 列的索引：

	```sql
	SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;
	```

2. 多列索引

    在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。

    ```sql
    SELECT film_id, actor_ id FROM sakila.film_actor
    WHERE actor_id = 1 AND film_id = 1;
    ```

3. 索引列的顺序

    让选择性最强的索引列放在前面。

    索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，每个记录的区分度越高，查询效率也越高。

    例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。

    ```sql
    SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,
    COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,
    COUNT(*)
    FROM payment;
       staff_id_selectivity: 0.0001
    customer_id_selectivity: 0.0373
                   COUNT(*): 16049
    ```

4. 前缀索引

    对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。

    前缀长度的选取需要根据索引选择性来确定。

5. 覆盖索引

    **覆盖索引**（Covering Index）,或者叫索引覆盖， 也就是平时所说的不需要回表操作。就是select的数据列只用从索引中就能够取得，不必读取数据行，MySQL可以利用索引返回select列表中的字段，而不必根据索引再次读取数据文件，换句话说**查询列要被所建的索引覆盖**。

    具有以下优点：

    - 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。
    - 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。
    - 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。

#### 相关问题

1. 为什么 MySQL 索引使用 B+ 树而不是 B 树？
    - **B+树的磁盘读写代价更低**：InnoDB 在把磁盘数据读入到内存时会以页为基本单位，页的大小是固定的，非叶子节点存储数据信息会使得每一个节点（即一个页）能存储的 key 的数量减少，从而使得树的深度增大，增大查询时的磁盘I/O次数，进而影响查询效率。
    - **B+树的查询效率更加稳定：B+树任何关键字的查找必须走一条从根结点到叶子结点的路，所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。**
    
2. 为什么不采用 Hash 的方式？

    使用 Hash 的方式，多个数据在存储关系上是完全没有任何顺序关系的，对于区间查询是无法直接通过索引查询的，就需要全表扫描，所以哈希索引只适用于等值查询的场景；因为存在哈希碰撞的问题，如果有大量重复键值的情况下，哈希索引的效率会很低。

### 4. 查询优化

#### 使用 Explain 进行分析

使用 **Explain** 关键字可以模拟优化器执行SQL查询语句，从而知道 MySQL 是如何处理你的 SQL 语句的。分析你的查询语句或是表结构的性能瓶颈

使用方式 Explain + sq l语句，例如：

```sql
EXPLAIN SELECT column1, column2 FROM tablename WHERE id=1;
```

使用 Explain 会展示的信息：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/explain.png" style="zoom:100%;" />

比较重要的字段有：

- select_type : 查询类型，有简单查询、联合查询、子查询等
- key : 使用的索引
- rows : 扫描的行数

#### 优化数据访问

1. 减少请求的数据量
    - 只返回必要的列：最好不要使用 SELECT * 语句。
    - 只返回必要的行：使用 LIMIT 语句来限制返回的数据。
    - 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。

2. 减少服务器端扫描的行数

    最有效的方式是使用索引来覆盖查询。

#### 重构查询方式

1. 切分大查询

    一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。

    ```sql
    DELETE FROM messages WHERE create < DATE_SUB(NOW(), INTERVAL 3 MONTH);
    rows_affected = 0
    do {
        rows_affected = do_query(
        "DELETE FROM messages WHERE create  < DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000")
    } while rows_affected > 0
    ```

2. 分解大连接查询

    将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有：

    - 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。
    - 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。
    - 减少锁竞争；
    - 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。
    - 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。

    ```sql
    SELECT * FROM tag
    JOIN tag_post ON tag_post.tag_id=tag.id
    JOIN post ON tag_post.post_id=post.id
    WHERE tag.tag='mysql';
    ```

    ```sql
    SELECT * FROM tag WHERE tag='mysql';
    SELECT * FROM tag_post WHERE tag_id=1234;
    SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904);
    ```

### 5. 数据类型

#### 整型

包含 BIT、BOOL、TINY INT、SMALL INT、MEDIUM INT、 INT、 BIG INT

TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。

INT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。

#### 浮点型

包含 FLOAT、DOUBLE、DECIMAL

FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAL 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。

FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/MySQL%E6%95%B4%E5%9E%8B%E5%92%8C%E6%B5%AE%E7%82%B9%E5%9E%8B.webp" style="zoom:67%;" />

#### 字符串类型

包含 CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB

char是固定长度，varchar长度可变。存储时，前者不管实际存储数据的长度，直接按 char 规定的长度分配存储空间；而后者会根据实际存储的数据分配最终的存储空间。

char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30) 就可以存储 30 个字符。

在进行存储和检索时，会保留 VARCHAR 末尾的空格，而会删除 CHAR 末尾的空格。

BLOB 与 TEXT 的区别：

- BLOB是一个二进制对象，可以容纳可变数量的数据。有四种类型的 BLOB：TINYBLOB、BLOB、MEDIUMBLOB和 LONGBLOB

- TEXT是一个不区分大小写的BLOB。四种TEXT类型：TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT。

- BLOB 保存二进制数据，TEXT 保存字符数据。

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/MySQL%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%B1%BB%E5%9E%8B.webp" style="zoom:67%;" />

#### 时间日期类型

包含 Date、DateTime、TimeStamp、Time、Year

MySQL 提供了两种相似的日期时间类型：DATETIME 和 TIMESTAMP。

1. DATETIME

    能够保存从 1000 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。它**与时区无关**。

    默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。

2. TIMESTAMP

    和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年。

    它**和时区有关**，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。

    MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。

    默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。

应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/MySQL%E6%97%A5%E6%9C%9F%E7%B1%BB%E5%9E%8B.webp" style="zoom:67%;" />

#### 其他数据类型

包含：BINARY、VARBINARY、ENUM、SET、Geometry、Point、MultiPoint、LineString、MultiLineString、Polygon、GeometryCollection等。

### 6. MySQL分表

#### 垂直拆分

垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。

垂直拆分的优点： 可以使得列数据变⼩，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护；
垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应⽤层进⾏Join来解决。此外，垂直分区会让事务变得更加复杂。

#### 水平拆分

水平拆分，又称为数据分片，是把一个表复制成同样表结构的不同表，然后把数据按照一定的规则划分，分别存储到这些表中，从而保证单表的容量不会太大，提升性能；当然这些结构一样的表，可以放在一个或多个数据库中。

水平拆分的策略：

- 使用MD5哈希，做法是对UID进行md5加密，然后取前几位（我们这里取前两位），然后就可以将不同的UID哈希到不同的用户表（user_xx）中了。
- 根据时间放入不同的表，比如：article_201601，article_201602。
- 根据ID的值放入对应的表，第一个表user_0000，第二个100万的用户数据放在第二 个表user_0001中，随用户增加，直接添加用户表就行了。

### 7. 主从复制

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6.webp" style="zoom:45%;" />

主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。

- **binlog 线程** ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。
- **I/O 线程** ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。
- **SQL 线程** ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。

步骤：

1. master 将改变记录到二进制日志（binary log）。这些记录过程叫做二进制日志事件，binary log events；
2. salve 将 master 的 binary log events 拷贝到它的中继日志（relay log）;
3. slave 重做中继日志中的事件，将改变应用到自己的数据库中。MySQL 复制是异步且是串行化的。

**读写分离**

主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。

读写分离能提高性能的原因在于：

- 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
- 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
- 增加冗余，提高可用性。

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。

<!-- 分页-->

<div style="page-break-after: always;"></div>

<!--分页-->

## SQL

### SQL 的执行顺序

[参考链接](https://blog.csdn.net/u014044812/article/details/51004754)

1. from 
2. join 
3. on 
4. where 
5. group by(开始使用select中的别名，后面的语句中都可以使用)
6.  avg,sum.... (聚集函数)
7. with {CUBE|ROLLUP}
8. having 
9. select 
10. distinct 
11. order by
12. limit

**所有的查询语句都是从from开始执行的，在执行过程中，每个步骤都会为下一个步骤生成一个虚拟表，这个虚拟表将作为下一个执行步骤的输入**。

执行过程：

1. 首先对from子句中的前两个表执行一个笛卡尔乘积，此时生成虚拟表 vt1（选择相对小的表做基础表）。 

2. 接下来便是应用 on 筛选器，on 中的逻辑表达式将应用到 vt1 中的各个行，筛选出满足 on 逻辑表达式的行，生成虚拟表 vt2。

3. 如果是outer join 那么这一步就将添加外部行，left outer join 就把左表在第二步中过滤的添加进来，如果是right outer join 那么就将右表在第二步中过滤掉的行添加进来，这样生成虚拟表 vt3。

4. 如果 from 子句中的表数目多余两个表，那么就将 vt3 和第三个表连接从而计算笛卡尔乘积，生成虚拟表，该过程就是一个重复1-3的步骤，最终得到一个新的虚拟表 vt3。

5. 对上一步生产的虚拟表引用where筛选器，生成虚拟表 vt4。

6. group by 子句将中的唯一的值组合成为一组，得到虚拟表 vt5。**如果应用了group by，那么后面的所有步骤都只能得到的vt5 的列或者是聚合函数（count、sum、avg等）。原因在于最终的结果集中只为每个组包含一行。**这一点请牢记。

7. 应用 cube 或者 rollup 选项，为 vt5 生成超组，生成 vt6。

8. 应用 having 筛选器，生成 vt7。**having 筛选器是第一个也是为唯一一个应用到已分组数据的筛选器**。

9. 处理 select 子句。将 vt7 中的在select中出现的列筛选出来生成 vt8。

10. 应用 distinct 子句，vt8 中移除相同的行，生成 vt9。事实上如果应用了group by子句那么 distinct 是多余的，原因在于，分组的时候是将列中唯一的值分成一组，同时只为每一组返回一行记录，那么所以的记录都将是不相同的。

11. 应用order by子句。按照order_by_condition排序vt9，此时返回的一个游标，而不是虚拟表。sql是基于集合的理论的，集合不会预先对他的行排序，它只是成员的逻辑集合，成员的顺序是无关紧要的。对表进行排序的查询可以返回一个对象，这个对象包含特定的物理顺序的逻辑组织。这个对象就叫游标。正因为返回值是游标，那么使用 order by 子句查询不能应用于表达式。

12. 应用top选项。此时才返回结果给请求者即用户。MySQL 的 LIMIT 子句可以被用于强制 SELECT 语句返回指定的记录数。LIMIT 接受一个或两个数字参数。参数必须是一个整数常量。**如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。初始记录行的偏移量是 0(而不是 1)**。

    > 在这有个比较重要的细节，对于包含 outer join 子句的查询，到底在 on 筛选器还是用 where 筛选器指定逻辑表达式呢？==on 和 where 的最大区别在于，如果在 on 应用逻辑表达式那么在第三步 outer join 中还可以把移除的行再次添加回来，而 where 的移除是最终的==。举个简单的例子，有一个学生表（班级,姓名）和一个成绩表（姓名,成绩），我现在需要返回一个 x 班级的全体同学的成绩，但是这个班级有几个学生缺考，也就是说在成绩表中没有记录。为了得到我们预期的结果我们就需要在 on 子句指定学生和成绩表的关系（学生.姓名=成绩.姓名）那么在执行第二步的时候，对于没有参加考试的学生记录就不会出现在 vt2 中，因为他们被 on 的逻辑表达式过滤掉了，但是我们用 left outer join 就可以把左表（学生）中没有参加考试的学生找回来，因为我们想返回的是 x 班级的所有学生，如果在 on 中应用学生.班级='x'的话，left outer join 会把不是 x 班级的所有学生记录找回，所以只能在where筛选器中应用学生.班级='x' 因为它的过滤是最终的。

    例如：

    <div style="display:flex;text-align:center;">
        <div style="flex:1;">
            <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/table1.png" style="zoom:100%;" />
        </div>
        <div style="flex:1;">
            <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/table2.png" style="zoom:100%;" />
        </div>
    </div>

    

    <div style="display:flex; text-align:center;">
        <div style="flex:1">
            <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/result1.png" style="zoom:100%;" />
        </div>
        <div style="flex:1">
            <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/result2.png" style="zoom:100%;" />
        </div>
    </div>
<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/result3.png" style="zoom:100%;" />

### where 和 having 的区别

​	**where 子句**的作用是在对查询结果进行分组前，将不符合where条件的行去掉，即**在分组之前过滤数据，条件中不能包含聚组函数**，使用where条件显示特定的行。
 	**having 子句**的作用是筛选满足条件的组，即**在分组之后过滤数据，条件中经常包含聚组函数**，使用having 条件显示特定的组，也可以使用多个分组标准进行分组。

### 索引的创建、删除和修改

- 创建：

  - 创建索引：`CREATE [UNIQUE] INDEX indexName ON mytable(columnname(length));`

    如果是 CHAR，VARCHAR 类型，length 可以小于字段实际长度；如果是 BLOB 和 TEXT 类型，必须指定 length。

  - 修改表结构(添加索引)：`ALTER table tableName ADD [UNIQUE] INDEX indexName(columnName)`

- 删除：`DROP INDEX [indexName] ON mytable;`

- 查看：`SHOW INDEX FROM table_name\G`       --可以通过添加 \G 来格式化输出信息。

- 使用 ALERT 命令

  - ALTER TABLE tablename ADD PRIMARY KEY (column_list):` 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。
  - `ALTER TABLE tbl_name ADD UNIQUE index_name (column_list` 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）。
  - `ALTER TABLE tbl_name ADD INDEX index_name (column_list)` 添加普通索引，索引值可出现多次。
  - `ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list)`该语句指定了索引为 FULLTEXT ，用于全文索引。

<!-- 分页-->

<div style="page-break-after: always;"></div>

<!--分页-->

## Redis

### 常见问题

#### Redis 的线程模型

**Redis 文件事件分派器（文件事件处理器的一部分）队列的消费是单线程的**。因为 Redis 完全是基于内存的操作，CPU 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是机器内存的大小或者网络带宽，既然单线程实现容易，那就顺理成章的采用单线程的方案了。

使用单线依然很快的原因：

- 基于内存操作
- 数据结构简单
- 使用多路复用和非阻塞 I/O
- 避免了不必要的线程上下文切换

在 Redis 4.0 中已经添加了多线程的支持，主要体现在大数据的异步删除功能上，例如 unlink key、flushdb async、flushall async 等，Redis 6.0 新增了多线程 I/O 的读写并发能力，用于更好的提高 Redis 的性能。

**Redis 6.0 后在网络 I/O 处理上使用的多线程**，主要实现思路是将主线程的 IO 读写任务拆分给一组独立的线程去执行，这样就可以使多个 socket 的读写可以并行化了，但 Redis 的命令依旧是由主线程串行执行的。

#### 基本数据类型及其实现结构

| 基本数据类型 |          实现结构          |
| :----------: | :------------------------: |
| 字符串string |     简单动态字符串SDS      |
|   列表list   |       压缩列表、链表       |
|  哈希表hash  |       压缩列表、字典       |
|   集合set    |       整数集合、字典       |
| 有序集合zset | 压缩列表、（字典和跳跃表） |

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis_datatype.png" style="zoom:60%;" />

#### 缓存和数据库双写的一致性

读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况，串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。

还有一种方式就是可能会暂时产生不一致的情况，但是发生的几率特别小，就是**先更新数据库，然后再删除缓存。**其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。

不一致的情况：

比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。

#### 缓存失效了怎么办

- **加锁重构缓存**

    第一个获取Lock的查DB，重构缓存，其他人二次查询缓存

    优点：简单有效，适用范围广
    缺点：阻塞其他线程，用户体验差

- **缓存降级**

    缓存降级简单的理解就是降低预期期望。比如双十一的时候很多人因为支付不成功而提示的稍后再试；还可以返回固定值或者提示稍后再试

    1）做备份缓存，不设置事件 

    2）返回固定值
    主备都无数据，一人去查DB，剩余人返回固定值
    主无数据，备有数据，一人查DB，剩余人查备份
    优点：灵活多变
    缺点：备份缓存数据可能不一致

### 缓存异常

#### 缓存雪崩

缓存雪崩是指同一时间大量缓存数据失效，导致所有的请求都落到数据库上，造成数据库短期内 CUP 和内存压力激增，严重时甚至造成宕机。

缓存集中失效的原因：redis服务器宕机或者缓存数据设置了相同的过期时间。

**解决方法**

- 使用 Redis Cluster 或者 Redis Sentinel(哨兵) 等方案实现 Redis 的高可用性
- 设置缓存过期时间时加上一个随机值，避免缓存在同一时间过期
- 如果 Redis 是集群部署，将热点数据均匀分布在不同的 Redis 库中也能避免全部失效
- 设置热点数据永不过期，有更新操作就更新缓存

#### 缓存穿透

缓存穿透是指请求缓存和数据库中都没有的数据，由于没有获取到缓存并且也不会写入缓存，导致这个不存在的数据每次都需要去数据库查询，有可能搞垮数据库，使整个服务瘫痪。

**解决方法**

- 使用布隆过滤器过滤掉非法请求
- 对不存在的数据也将其缓存到 redis 中，设置key，value值为null，并设置一个较短的过期时间，防止攻击用户反复用同一个key暴力攻击。
- 接口层增加校验，对不合法参数进行拦截

#### 缓存击穿

缓存击穿是指某个 key 的缓存非常热门，有很高的并发一直在访问，当这个 key 在失效的瞬间，大量的该 key 的请求会落到数据库上，压垮数据库。

**解决方法**

- 设置热点数据永不过期
- 加互斥锁，通过 redis 的 setnx 实现互斥锁

> 总结：雪崩是大面积的key缓存失效；穿透是 redis 和数据库里都不存在这个数据；击穿是 redis 某一个热点key突然失效

#### 缓存预热



### 1. 简单动态字符串（SDS）

#### 1.1 实现

Redis使用`sdshdr`结构来表示一个`SDS`值：

```c
struct sdshdr{
    // 记录buf数组中已使用的字节数量，也是字符串的长度
    int len;
    // 记录buf数组未使用的字节数量
    int free;
    // 字节数组，用于保存字符串
    char buf[];
}
```

例如：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis_sds.jpg" style="zoom:80%;" />

​	SDS 遵循 C 字符串以空字符结尾的惯例，**保存空字符的1字节空间不计算在 SDS 的 len 属性里面**，并且为空字符分配额外的1字节空间，以及添加空字符到字符串末尾等操作，都是由 SDS 函数自动完成的。

#### 1.2 SDS 与 C 字符串的区别

Redis **只会使用C字符串作为字面量**，在大多数情况下，Redis使用 SDS 作为字符串表示。

|                  SDS                   |                C 字符串                |
| :------------------------------------: | :------------------------------------: |
|  获取字符串长度的时间复杂度为 $ O(1)$  |  获取字符串长度的时间复杂度为 $ O(n)$  |
|    API 是安全的，不会造成缓冲区溢出    |  API 是不安全的，可能会造成缓冲区溢出  |
| 修改字符串长度N次最多需要N次内存重分配 | 修改字符串长度N次必然需要N次内存重分配 |
|        可以保存文本或二进制数据        |            只能保存文本数据            |
|  可以使用一部分`<string.h>`库中的函数  |   可以使用所有`<string.h>`库中的函数   |

##### 常数复杂度获取字符串长度

- C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为$O(N)$。
- SDS 在 len 属性中记录了 SDS 本身的长度，所以获取一个SDS长度的复杂度仅为$O(1)$。

通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从$O(N)$降低到了$O(1)$，这确保了获取字符串长度的工作不会成为 Redis 的性能瓶颈。

##### 杜绝缓冲区溢出

- C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出（buffer overflow）

    > 举个例子，假设程序里有两个在内存中紧邻着的C字符串 s1 和 s2，其中 s1 保存了字符串"Redis"，而 s2 则保存了字符串"MongoDB"。现在执行`strcat(s1, " Cluster");`在 s1 后面添加字符串，如果在执行 strcat 函数之前没有给 s1 分配足够的内存，那么在 strcat 函数执行之后，s1 的数据将溢出到 s2 所在的空间中，导致 s2 保存的内容被意外地修改。

- SDS的空间分配策略完全杜绝了发生缓冲区溢出的可能性：当需要对 SDS 进行修改时，会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，会自动将SDS的空间扩展至执行修改所需的大小，然后才执行实际的修改操作。

##### 减少修改字符串时带来的内存重分配次数

- C 字符串的长度和底层数组的长度之间存在着关联性，即一个包含了N个字符的 C 字符串的底层实现总是一个N+1个字符长的数组，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作。

- SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联，同时，通过未使用空间，SDS 实现了**空间预分配**和**惰性空间释放**两种优化策略。

    - **空间预分配用于优化 SDS 的字符串增长操作**：当增长 SDS 保存的字符串，并需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。

        额外分配的未使用空间数量由以下公式决定：

        1. 当修改后的 SDS 的长度（也即是 len 属性的值）将小于1MB，那么程序分配和len属性同样大小的未使用空间，这时SDS len属性的值将和 free 属性的值相同。
        2. 当修改后的 SDS 的长度将大于等于1MB，那么程序会分配1MB的未使用空间。

        通过空间预分配策略，Redis可以**减少连续执行字符串增长操作所需的内存重分配次数**。

    - **惰性空间释放用于优化 SDS 的字符串缩短操作**：当需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 free 属性将这些字节的数量记录起来，并等待将来使用。

        通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。

##### 二进制安全

- C 字符串中的字符必须符合某种编码（比如ASCII），**并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾**，这些限制使得C字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。
- SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，所以 SDS 可以保存各种特殊格式的数据，这就使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据。

##### 兼容部分C字符串函数

通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用`＜string.h＞`函数库，从而避免了不必要的代码重复。

### 2. 链表

链表节点的结构：

```c
typedef strcut listNode{
    // 前置节点
    strcut listNode  *pre;
    // 后置节点
    strcut listNode  *next;
    // 节点的值
    void  *value;
}listNode;
```

链表结构：

```C
typedef struct list{
    // 表头结点
    listNode  *head;
    // 表尾节点
    listNode  *tail;
    // 链表长度
    unsigned long len;
    // 节点值复制函数，复制链表节点保存的值
    void *(*dup) (viod *ptr);
    // 节点值释放函数，释放链表节点保存的值
    void  (*free) (viod *ptr);
    // 节点值对比函数，用于对比链表节点所保存的值与另一个传入的值是否相等
    int (*match) (void *ptr,void *key);
}list;
```

Redis 链表的具体结构如下图：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/rdis-%E9%93%BE%E8%A1%A8.png" style="zoom:60%;" />

##### Redis链表的特性

- 双端：链表节点带有 prev 和 next 指针，获取某个节点的前置节点和后置节点的复杂度都是$O(1)$。

- 无环：表头节点的 prev 指针和表尾节点的next指针都指向 NULL，对链表的访问以 NULL 为终点。

- 带表头指针和表尾指针：通过 list 结构的 head 指针和tail指针，程序获取链表的表头节点和表尾节点的复杂度为$O(1)$。

- 带链表长度计数器：程序使用 list 结构的 len 属性来对list持有的链表节点进行计数，程序获取链表中节点数量的复杂度为$O(1)$。

- 多态：链表节点使用 void* 指针来保存节点值，并且可以通过list结构的 dup、free、match 三个属性为节点值设置类型特定函数，所以**链表可以用于保存各种不同类型的值**。

总的来说就是**双向无环，$O(1)$时间获取头结点、尾结点和链表长度，可以保存各种不同类型的值**。

### 3. 字典

#### 3.1 实现

Redis的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。

##### 哈希表

Redis 中哈希表的结构如下：

```c
typedef struct dictht{
    // 哈希表数组
    dictEntry **table;  
    // 哈希表大小
    unsigned long size;    
    // 哈希表大小掩码，用于计算索引值
    // 总是等于size-1
    unsigned long sizemark;     
    // 哈希表已有节点数量
    unsigned long used;
}dictht
```

​	table 是一个数组，数组中的每一个元素都是一个指向`dictEntry`结构的指针，每一个`dictEntry`结构保存着一个键值对。一个大小为4的空哈希表如下图所示：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-%E5%93%88%E5%B8%8C%E8%A1%A8.png" style="zoom:65%;" />

##### 哈希表节点

哈希表节点结构如下：

```c
typedef struct dictEntry {
    // 键
    void *key;
    // 值
    union {
        void *value;
        uint64_tu64;
        int64_ts64;
    }v;    
    // 指向下个哈希节点，组成链表
    struct dictEntry *next;
}dictEntry;
```

- v 属性保存着键值对中的值，它可以是一个指针，或者是一个 unit64_tu64 整数，或者是一个 int64_ts64 整数。
- next 指针将多个哈希值相同的键值对连接在一起，以此来解决键冲突问题，例如下图：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-%E5%93%88%E5%B8%8C%E8%A1%A8%E8%8A%82%E7%82%B9.png" style="zoom:50%;" />

##### 字典

字典结构：

```c
typedef struct dict {
    // 类型特定函数
    dictType *type;
    // 私有数据
    void *privdata;
    // 哈希表
    dictht ht[2];
    // rehash索引
    // 当rehash不进行时，值为-1
    int rehashidx;  
}dict;
```

其中 `dictType `定义如下：

```c
typedef struct dictType{
    // 计算哈希值的函数
    unsigned int (*hashFunction)(const void * key);
    // 复制键的函数
    void *(*keyDup)(void *private, const void *key);
    // 复制值d的函数
    void *(*valDup)(void *private, const void *obj);  
    // 对比键的函数
    int (*keyCompare)(void *privdata , const void *key1, const void *key2)
    // 销毁键的函数
    void (*keyDestructor)(void *private, void *key);
    // 销毁值的函数
    void (*valDestructor)(void *private, void *obj);  

}dictType
```

- type 属性是一个指向 dictType 结构的指针，每个 dictType 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。
- privdata 属性则保存了需要传给那些类型特定函数的可选参数。
- ht 属性是一个包含两个项的数组，数组中的每个项都是一个 dictht 哈希表，**一般情况下，字典只使用 ht[0] 哈希表，ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用**。
- rehashidx 记录了 rehash 目前的进度，如果目前没有在进行 rehash，那么它的值为-1。

普通状态下的 字典如下图所示：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-%E5%AD%97%E5%85%B8.png" style="zoom:40%;" />

#### 3.2 rehash

​	为了让哈希表的负载因子（loadfactor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或者收缩，这需要通过执行rehash（重新散列）操作来完成，Redis 对字典的哈希表执行rehash 的步骤如下：

1. 为字典的 ht[1] 哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及 ht[0] 当前包含的键值对数量（也即是ht[0].used 属性的值）：
    - 如果执行的是扩展操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used*2 的$2^n$（2的n次方幂）；
    - 如果执行的是收缩操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used 的$2^n$。
2. 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面：rehash 指的是重新计算键的哈希值和索引值，然后将键值对放置到ht[1]哈希表的指定位置上。
3. 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后（ht[0] 变为空表），释放 ht[0]，将 ht[1] 设置为 ht[0]，并在 ht[1] 新创建一个空白哈希表，为下一次 rehash 做准备。

##### 渐进式 rehash

​	上面提到将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面，但是，这个 rehash 动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成的。

**原因：数据量如果过大的话，一次性rehash需要庞大的计算量，这很可能导致服务器一段时间内停止服务**。

渐进式 rehash 的过程：

1. 为 ht[1] 分配空间，**让字典同时持有 ht[0] 和 ht[1] 两个哈希表**。
2. 在字典中维持一个索引计数器变量 rehashidx，并将它的值设置为0，表示 rehash 工作正式开始。
3. 在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，**程序除了执行指定的操作以外，还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1]**，==操作完成后 rehash 加一==。
4. 随着字典操作的不断执行，最终在某个时间点上，ht[0]的所有键值对都会被 rehash 至 ht[1]，这时程序将 rehashidx 属性的值设为-1，表示 rehash 操作已完成。

在进行渐进式 rehash 的过程中，字典会同时使用 ht[0] 和 ht[1] 两个哈希表，所以在渐进式 rehash 进行期间，字典的删除(delete)、查找(find)、更新(update)等操作会在两个哈希表上进行。例如要查找一个键的话，**服务器会优先查找 ht[0]，如果不存在，再查找 ht[1]**。此外当执行**新增操作**时，新的键值对**一律保存到 ht[1]**，不再对 ht[0] 进行任何添加操作，以保证 ht[0] 的键值对数量只减不增，直至变为空表。

### 4. 跳跃表(skiplist)

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/skipList.png" style="zoom:60%;" />

跳跃表在插入节点的时候，**抛硬币来决定新插入结点的层数**：每次我们要插入一个结点的时候，就来抛硬币，如果抛出来的是**正面**，则继续抛，直到出现**负面**为止，统计这个过程中出现正面的**次数**，这个次数作为结点跨越的层数。

#### 4.1 实现

Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成。其中 **zskiplist 保存跳跃表的信息**(表头，表尾节点，长度)，**zskiplistNode 则表示跳跃表的节点**。

##### 跳跃表节点

```c
typeof struct zskiplistNode {
    // 层
    struct zskiplistLevel {
        // 前进指针
        struct zskiplistNode *forward;
        // 跨度
        unsigned int span;
    } level[];
    // 后退指针
    struct zskiplistNode *backward;
    // 分值
    double score;
    // 成员对象
    robj *obj;
} zskiplistNode;
```

- 层：level数组可以包含多个元素，每个元素都包含一个指向其他节点的指针。每次创建一个新跳跃表节点的时候，程序都==根据幂次定律==（power law，越大的数出现的概率越小，就像上面提到的抛硬币）==随机生成一个介于1和32之间的值作为 level 数组的大小==，这个大小就是层的“高度”。

    每个层都有一个指向表尾方向的前进指针（level[i].forward属性），用于从表头向表尾方向访问节点。

    层的跨度（level[i].span属性）用于记录两个节点之间的距离：两个节点之间的跨度越大，它们相距得就越远；指向NULL的所有前进指针的跨度都为0，因为它们没有连向任何节点。**跨度实际上是用来计算排位**（rank）的：在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，得到的结果就是目标节点在跳跃表中的排位。

- 后退指针：节点的后退指针（backward属性）用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。

- 分值和成员对象

    节点的分值（score属性）是一个double类型的浮点数，跳跃表中的所有节点都按分值从小到大来排序。

    成员对象：节点的成员对象（obj属性）是一个指针，它指向一个字符串对象，而字符串对象则保存着一个SDS值。

    <span style="text-decoration: underline wavy red;">在同一个跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值却可以是相同的</span>：分值相同的节点将按照成员对象在字典序中的大小来进行排序，成员对象较小的在前。

##### 跳跃表

```c
typeof struct zskiplist {
    // 表头节点，表尾节点
    struct skiplistNode *header,*tail;
    // 表中节点数量
    unsigned long length;
    // 表中最大层数
    int level;
} zskiplist;
```

- level：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内）。
- length：记录跳跃表的长度，跳跃表目前包含节点的数量（表头节点不计算在内）。

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-skipList.png" style="zoom:65%;" />

> 注意：表头节点和其他节点的构造是一样的，表头节点也有后退指针、分值和成员对象，不过表头节点的这些属性都不会被用到，所以图中省略了这些部分，只显示了表头节点的各个层。

### 5. 整数集合(intset)

#### 5.1 实现

​	整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构，它可以保存类型为 int16_t、int32_t 或者 int64_t 的整数值，它保证集合中元素是**不会出现重复**的，并且是==**有序的(从小到大排序)**==，其结构如下：

```c
typeof struct intset {
    // 编码方式
    unit32_t encoding;
    // 集合包含的元素数量
    unit32_t lenght;
    // 保存元素的数组
    int8_t contents[];
} intset;
```

- contents 数组是整数集合的底层实现：整数集合的每个元素都是 contents 数组的一个数组项（item），==**各个项在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复项**==。
- length 属性记录了整数集合包含的元素数量，也即是 contents 数组的长度.
- encoding 属性：虽然 contents 属性声明为 int8_t 类型的数组，但其并不存储 int8_t 类型的数据，contents数组的真正类型取决于encoding属性的值。
    - 值为 INTSET_ENC_INT16，数组里的每个项都是一个 int16_t 类型的整数值（最小值为-32768，最大值为32767）。
    - 值为 INTSET_ENC_INT32，数组里的每个项都是一个 int32_t 类型的整数值（最小值为-2147483648，最大值为2147483647）。
    - 值为 INTSET_ENC_INT64，数组里的每个项都是一个 int64_t 类型的整数值（最小值为-9223372036854775808，最大值为9223372036854775807）。

intset 示例：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-intset.png" style="zoom:75%;" />

#### 5.2 升级

当一个新的元素添加到集合里面，并且该元素超出集合当前编码能存放的整数值范围，这时就需要进行编码升级，升级步骤如下：

- 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。
- 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性质不变。
- 将新元素添加到底层数组里面。

因为每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中已有的所有元素进行类型转换，所以**向整数集合添加新元素的时间复杂度为$O(n)$**。

因为引发升级的新元素的长度总是比整数集合现有所有元素的长度都大，所以这个==新元素的值要么就大于所有现有元素，要么就小于所有现有元素==。

**整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。**

### 6. 压缩列表(ziplist)

压缩列表是 Redis <span style="text-decoration: underline wavy red;">为了节约内存而开发的</span>，是由一系列特殊编码的**连续内存块组成的顺序型（sequential）数据结构**，在 Redis 中并没有 ziplist 结构体的定义。

压缩列表的组成部分如下：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-ziplist.png" style="zoom:70%;" />

各个属性的类型、长度、用途如下：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-ziplist-attribute.png" style="zoom:65%;" />

压缩列表节点的构成：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-zlentry.png" style="zoom:65%;" />

> 上述结构在Redis 中是不存在对应的结构体的，Redis 中存在 zlentry 结构体的定义，但一般倾向于使用上述逻辑结构表示压缩列表节点

- previous_entry_length 属性以字节为单位，记录了压缩列表中前一个节点的长度。长度可以是1字节或者5字节，如果前一节点的长度小于254字节，那么就用一个字节存储；如果前一节点的长度大于等于254字节，会用五个字节存储并将第一个字节的值存储为固定值 0xFE（十进制254） 用于区分。
- encoding 属性记录了节点的 content 属性所保存数据的类型以及长度
    - 一字节、两字节或者五字节长，值的最高位为00、01或者10的是字节数组编码，表示 content 保存着字符数组，数组长度由编码除去最高两位之后的其他位记录。
    - 一字节长，值的最高位以11开头的是整数编码，表示content 保存着整数值，整数值的类型和长度由编码除去最高两位之后的其他位记录。
- content 属性负责保存节点的值，节点值可以是一个字节数组或者整数，值的类型和长度由节点的 encoding 属性决定。

> 压缩列表从表尾节点**倒序遍历**，首先指针通过 zltail 偏移量指向表尾节点，然后通过**节点记录的前一个节点的长度依次向前遍历访问整个压缩列表**。

#### 连锁更新

假设在一个压缩列表中，有多个连续的、长度介于250字节到253字节之间的节点 e1 至 eN，它们的 previous_entry_length 属性都是1字节长的。如果我们将一个长度大于等于254字节的新节点 new 设置为压缩列表的表头节点，那么 new 将成为 e1 的前置节点，同时 e1 节点的previous_entry_length属性从原来的1字节长扩展为5字节长，e1 的长度就变成了介于254字节至257字节之间，因此 e2 节点也需要进行扩展，然后 e3 节点也需要扩展，一直到 en 为止，这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”。 

### 7. 对象

​	Redis 使用对象来表示数据库中的键和值，每个对象都由一个 redisObject 结构表示，该结构中和保存数据有关的三个属性分别是 type 属性、encoding 属性和 ptr 属性：

```c
typedef struct redisObject {
    // 类型
    unsigned type:4;
    // 编码
    unsigned encoding:4;
    // 指向底层实现数据结构的指针
    void *ptr;
    // 对象最后一次被命令程序访问的时间
    unsigned lru:LRU_BITS;
    // 引用计数
    int refcount;
} robj;
```

- **类型**

    对于Redis数据库保存的键值对来说，==键总是一个字符串对象，而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象的其中一种==。

    **对一个 key 执行 TYPE 命令**时，命令**返回**的结果为 key 对应的 **value 的类型，而不是 key 的类型**。

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redisObject-type.png" style="zoom:70%;" />

- **编码**

    每种类型的对象都至少使用了两种不同的编码，具体如下图：

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redisObject-encoding.png" style="zoom:60%;" />

#### 7.1 字符串对象(string)

   字符串对象有三种编码格式：

- int：字符串对象保存的是整数值，且该整数值可以用 long 类型来表示。

    > 浮点数在 Redis 中是作为字符串值来存储的，需要的时候再将字符串转换为数值。

- embstr：保存的是字符串值，且字符串的长度小于等于32字节。
- raw：保存的是字符串值，且字符串的长度大于32字节。

常用命令：

```c
SET key value  
    // 设置指定 key 的值
GET key  
    // 获取指定 key 的值。
GETRANGE key start end
    // 返回 key 中字符串值的子字符
DEL key
    // 删除指定 key 的值
STRLEN key
	// 返回 key 所储存的字符串值的长度。
INCR key
	// 将 key 中储存的数字值增一。
INCRBY key increment
	// 将 key 所储存的值加上给定的增量值（increment） 。
INCRBYFLOAT key increment
	// 将 key 所储存的值加上给定的浮点增量值（increment） 。
DECR key
	// 将 key 中储存的数字值减一。
DECRBY key decrement
	// 将key 所储存的值减去给定的减量值（decrement） 。
```

#### 7.2 列表对象(list)

列表对象有两种编码方式

- ziplist 编码：列表对象保存的所有字符串元素的长度都小于64字节，且列表对象保存的元素数量小于512个。

- linkedlist 编码：不满足 ziplist 编码条件的使用 linkedlist 编码。

对于使用 ziplist 编码的列表对象来说，当使用 ziplist 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行。

常用命令：

```c
LPUSH/RPUSH key value1 [value2]
	// 将一个或多个值按所给顺序逐次插入到列表头部/尾部
LPOP/RPOP key
	// 移出并获取列表的第一个元素/最后一个元素
LINDEX key index
	// 通过索引获取列表中的元素
LLEN key
	// 获取列表长度
```

#### 7.3 哈希对象(hash)

哈希对象有两种编码方式：

- ziplist 编码：哈希对象保存的所有键值对的键和值的字符串长度都小于64字节 && 哈希对象保存的键值对数量小于512个

    ziplist 编码的哈希对象**使用压缩列表作为底层实现**，每当有新的键值对要加入到哈希对象时，程序会先将保存了键的压缩列表节点推入到压缩列表表尾，然后再将保存了值的压缩列表节点推入到压缩列表表尾。

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-hash-ziplist.png" style="zoom:70%;" />

- hashtable 编码：不满足上述两个条件的哈希对象需要使用 hashtable 编码。

    hashtable 编码的哈希对象**使用字典作为底层实现**，哈希对象中的每个键值对都使用一个字典键值对（dictEntry）来保存，dictEntry 的 key、value 都是字符串对象，分别保存了键值对的键、值。

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-hash-hashtable.png" style="zoom:70%;" />

如同列表对象，哈希对象同样存在编码转换。

```c
HSET key field value
	// 将哈希表 key 中的字段 field 的值设为 value
HDEL key field1 [field2]
	// 删除一个或多个哈希表字段
HEXISTS key field
	// 查看哈希表 key 中，指定的字段是否存在
HGET key field
	// 获取存储在哈希表中指定字段的值
HGETALL key
	// 获取在哈希表中指定 key 的所有字段和值
HKEYS key
	// 获取所有哈希表中的字段
HMGET key field1 [field2]
	// 获取所有给定字段的值
HMSET key field1 value1 [field2 value2 ]
	// 同时将多个 field-value (域-值)对设置到哈希表 key 中。
```

#### 7.4 集合对象(set)

集合对象编码：

- intset 编码：集合对象保存的所有元素都是整数值 && 集合对象保存的元素数量不超过512个

    intset 编码的集合对象**使用整数集合作为底层实现**，集合对象包含的所有元素都被保存在整数集合里面。

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-Set-inset.png" style="zoom:60%;" />

- hashtable 编码：不满足上述两个条件的集合对象需要使用 hashtable 编码

    hashtable 编码的集合对象**使用字典作为底层实现**，字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，而字典的值则全部被设置为 NULL。

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-Set-hashtable.png" style="zoom:60%;" />

常用命令：

```c
SADD key member1 [member2]
	// 向集合添加一个或多个成员
SCARD key
	// 获取集合的成员数
SPOP key
	// 移除并返回集合中的一个随机元素
SISMEMBER key member
	// 判断 member 元素是否是集合 key 的成员
SMEMBERS key
	// 返回集合中的所有成员
SREM key member1 [member2]
	// 移除集合中一个或多个成员
SUNION key1 [key2]
	// 返回所有给定集合的并集
```

#### 7.5 有序集合对象(sorted set)

有序集合对象编码：

- ziplist 编码：有序集合保存的所有元素成员的长度都小于64字节 && 有序集合保存的元素数量小于128个

    ziplist 编码的有序集合对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员（member），而第二个元素则保存元素的分值（score）。压缩列表内的集合元素按分值从小到大进行排序。

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-sortedset-ziplist.png" style="zoom:90%;" />

- skiplist 编码：不能满足上述两个条件的有序集合对象将使用 skiplist 编码。

    skiplist 编码的有序集合对象使用 zset 结构作为底层实现，一个 zset 结构同时包含一个字典和一个跳跃表：

    ```c
    typedef struct zset {
        zskiplist *zsl;
        dict *dict;
    } zset;
    ```

    zsl 跳跃表按分值从小到大保存了所有集合元素，每个跳跃表节点都保存了一个集合元素：跳跃表节点的object属性保存了元素的成员，而跳跃表节点的score属性则保存了元素的分值。

    dict 字典为有序集合创建了一个从成员到分值的映射，字典中的每个键值对都保存了一个集合元素：字典的键保存了元素的成员，而字典的值则保存了元素的分值。

    **跳跃表结构使得可以对有序集合进行范围型操作，比如 ZRANK、ZRANGE 等命令；字典结构使得可以用$O(1)$复杂度查找给定成员的分值。**

    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-sortedset-skiplist.png" style="zoom:65%;" />

    > 上图只是为了展示方便，实际中，==字典和跳跃表通过指针来共享相同元素的成员和分值==，并不会造成任何数据重复，也不会因此而浪费任何内存。

有序集合**每个元素的成员都是一个字符串对象**，而**每个元素的分值都是一个double类型的浮点数**。

**同时使用使用跳跃表和字典来实现有序集合的原因：**

- 只使用字典，$O(1)$复杂度查找给定成员的分值，但每次在执行范围型操作需要对字典的元素进行排序，这需要至少$O(NlogN)$ 的时间复杂度和$O(N)$ 的空间复杂度。
- 只使用跳跃表，能够快速的实现范围操作，但根据成员查找分值这一操作的复杂度为$O(logN)$。

.常用命令：

```c
ZADD key score1 member1 [score2 member2]
	// 向有序集合添加一个或多个成员，或者更新已存在成员的分数
ZCARD key
	// 获取有序集合的成员数
ZCOUNT key min max
	// 计算在有序集合中指定区间分数的成员数
ZLEXCOUNT key min max
	// 在有序集合中计算指定字典区间内成员数量
ZRANGE key start stop [WITHSCORES]
	// 通过索引区间返回有序集合指定区间内的成员，返回整个集合索引为 0 -1
ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT]
	// 通过分数返回有序集合指定区间内的成员
ZRANK key member
	// 返回有序集合中指定成员的索引
ZREM key member [member ...]
	// 移除有序集合中的一个或多个成员
ZSCORE key member
	// 返回有序集中，成员的分数值，返回值是字符串
```

#### 补充

- 服务器**在执行某些命令之前，会先检查给定键的类型能否执行指定的命令**，而检查一个键的类型就是检查键的值对象的类型。

- Redis的对象系统带有**引用计数**实现的内存回收机制，创建一个新对象，计数值初始化为1，对象被一个新程序使用时，计数加一，不再被一个程序使用时计数减一，计数为0，对象被回收。

- Redis会共享值为0到9999的字符串对象，被共享的值对象会相应的增加增加的引用计数。

    不共享包含字符串的对象是因为，共享一个对象之前需要先检查给定的共享对象和键想创建的目标对象是否完全相同，一个共享对象保存的值越复杂，验证共享对象和目标对象是否相同所需的复杂度就会越高，消耗的CPU时间也会越多。

- 对象会记录自己的最后一次被访问的时间，这个时间可以用于计算对象的空转时间。

#### 7.6 HyperLogLog、GEO 与 Stream

- HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。

    > 基数：
    >
    > 例如，数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。

    基本命令：

    ```c
    PFADD key element [element ...]
    	// 添加指定元素到 HyperLogLog 中。
    PFCOUNT key [key ...]
    	// 返回给定 HyperLogLog 的基数估算值，如果多个 HyperLogLog 则返回基数估值之和。
    PFMERGE destkey sourcekey [sourcekey ...]
    	// 将多个 HyperLogLog 合并为一个 HyperLogLog
    ```

- GEO 主要用于存储地理位置信息，并对存储的信息进行操作。

    常用操作：

    ```c
    GEOADD key longitude latitude member [longitude latitude member ...]
        // 添加地理位置的坐标
    GEOPOS key member [member ...]
        // 获取地理位置的坐标
    GEODIST key member1 member2
        // 计算两个位置之间的距离
    GEORADIUS key longitude latitude radius m|km|ft|mi
        // 根据用户给定的经纬度坐标来获取指定范围内的地理位置集合
    GEORADIUSBYMEMBER key member radius m|km|ft|mi
        // 根据储存在位置集合里面的某个地点获取指定范围内的地理位置集合。
    GEOHASH key member [member ...]
        // 返回一个或多个位置对象的 geohash 值。
    ```

- Stream 主要用于消息队列（MQ，Message Queue）。

    发布订阅 (pub/sub) 可以分发消息，但无法记录历史消息。而 Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。

#### 7.7 各个数据类型的应用场景

<div>
    <table style="text-align: center;">
        <tr>
            <th style="width:100px">类型</th>
            <th style="width:190px">简介</th>
            <th style="width:350px">特点</th>
            <th style="width:300px">应用场景</th>
        </tr>
        <tr>
            <td>字符串string</td>
            <td>---</td>
            <td style="word-wrap:break-word;word-break:break-all;">---</td>
            <td style="word-wrap:break-word;word-break:break-all;">常规key-value缓存，计数器：粉丝数、访问量</td>
        </tr>
        <tr>
            <td>哈希hash</td>
            <td>键值对集合,即编程语言中的Map类型</td>
            <td style="word-wrap:break-word;word-break:break-all;">适合存储结构化的数据，比如一个对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去)</td>
            <td style="word-wrap:break-word;word-break:break-all;">存储、读取、修改用户属性</td>
        </tr>
		<tr>
            <td>列表list</td>
            <td>链表(双向链表)</td>
            <td style="word-wrap:break-word;word-break:break-all;">增删快,提供了操作某一段元素的API</td>
            <td style="word-wrap:break-word;word-break:break-all;">最新消息排行等功能(比如朋友圈的时间线)，关注列表、粉丝列表，消息队列</td>
        </tr>
		<tr>
            <td>集合set</td>
            <td>一般是hashtable实现，元素不重复</td>
            <td style="word-wrap:break-word;word-break:break-all;">1.hashtable实现的话添加、删除,查找的复杂度都是O(1) 2.有求交集、并集、差集等操作</td>
            <td style="word-wrap:break-word;word-break:break-all;">1.共同好友、共同关注 2.利用唯一性,统计访问网站的所有独立ip 3、好友推荐时,根据tag求交集,大于某个阈值就可以推荐</td>
        </tr>
		<tr>
            <td>有序集合sorted set</td>
            <td>将Set中的元素增加一个权重参数score,元素按score有序排列</td>
            <td style="word-wrap:break-word;word-break:break-all;">---</td>
            <td style="word-wrap:break-word;word-break:break-all;">1.排行榜 2.带权重的消息队列</td>
        </tr>
		<tr>
            <td>GEO</td>
            <td>---</td>
            <td style="word-wrap:break-word;word-break:break-all;">---</td>
            <td style="word-wrap:break-word;word-break:break-all;">附近的人、附近的酒店</td>
        </tr>
    </table>
</div>

### 8. 过期键

#### 8.1 设置键的过期时间

Redis有四个不同的命令可以用于设置键的生存时间（键可以存在多久）或过期时间（键什么时候会被删除）：

- 设置键可以存在多久
    - EXPIRE＜key＞＜ttl＞命令用于将键key的生存时间设置为 ttl **秒**。
    - PEXPIRE＜key＞＜ttl＞命令用于将键key的生存时间设置为 ttl **毫秒**。
- 设置键的过期时间：
    - EXPIREAT＜key＞＜timestamp＞命令用于将键key的过期时间设置为 timestamp 所指定的秒数时间戳。
    - PEXPIREAT＜key＞＜timestamp＞命令用于将键key的过期时间设置为 timestamp 所指定的毫秒数时间戳。

**TTL 命令和 PTTL 命令**接受一个带有生存时间或者过期时间的键，**返回这个键的剩余生存时间**，也就是，返回距离这个键被服务器自动删除还有多长时间。

**PERSIST \<key> 命令可以移除一个键的过期时间**。

#### 8.2 过期键的删除策略

有三种过期删除策略：

- **定时删除**：在设置键的过期时间的同时，创建一个定时器（timer），到过期时间定时器会立即执行对键的删除操作。

    优点：可以立即清除过期的数据，对内存很友好。

    缺点：对 CPU 时间不友好，在过期键比较多的情况下，会占用相当一部分的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。

- **惰性删除**：放任键过期不管，但每次访问键的时候都检查取得的键是否过期，如果过期的话，就删除该键；如果没有过期，就返回该键。

    优点：对CPU时间是最友好的

    缺点：对内存不友好，极端情况可能出现大量的过期 key 没有再次被访问，从而不会被清除，占用大量内存。

- **定期删除**：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。

**Redis中同时使用了==惰性删除==和==定期删除==两种过期策略。**

#### 8.3 AOF、RDB和复制功能对过期键的处理

- RDB 对过期键的处理

    - 生成 RDB 文件时，程序会对数据库中的键进行检查，**已过期的键不会被保存到新创建的RDB文件中**。
    - 载入 RDB 文件时
        - 如果服务器**以主服务器模式运行**，那么在载入RDB文件时，程序会对文件中保存的键进行检查，未过期的键会被载入到数据库中，而**过期键则会被忽略**。
        - 如果服务器以从服务器模式运行，那么在载入RDB文件时，文件中保存的所有键，**不论是否过期，都会被载入到数据库中**。但主从服务器同步时，从服务器的数据会被清空。

- AOF 对过期键的处理

    - AOF 文件写入时，如果某个键已经过期但未被删除，它不会对 AOF 造成影响；如果某个键过期且已被删除，程序会向 AOF 文件追加（append）一条DEL命令，来显式地记录该键已被删除。
    - 在执行 AOF 重写的过程中，程序会对数据库中的键进行检查，**已过期的键不会被保存到重写后的 AOF 文件中**。

- 复制对过期键的处理

    从服务器的过期键删除动作由主服务器控制：

    - 主服务器在删除一个过期键之后，会显式地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键。
    - 从服务器在执行客户端发送的读命令时，即使碰到过期键也不会将过期键删除，而是继续像处理未过期的键一样来处理过期键。
    - 从服务器只有在接到主服务器发来的DEL命令之后，才会删除过期键。

### 9. 内存管理

#### 数据淘汰策略

当 Redis 所用内存达到 maxmemory 上限时会触发相应的数据淘汰策略

<table style="text-align: center;width:650px">
    <thead>
        <tr>
            <th style="width:150px;">策略</th>
            <th>描述</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>volatile-lru</td>
            <td>从已设置过期时间的数据集中挑选最近最少使用(least recently used)的数据淘汰，直到腾出足够空间为止</td>
        </tr>
        <tr>
            <td>volatile-ttl</td>
            <td>从已设置过期时间的数据集中挑选剩余时间短(time to live)的数据淘汰，直到腾出足够空间为止</td>
        </tr>
        <tr>
            <td>volatile-random</td>
            <td>从已设置过期时间的数据集中任意选择数据淘汰，直到腾出足够空间为止</td>
        </tr>
        <tr>
            <td>allkeys-lru</td>
            <td>从所有数据集中挑选最近最少使用的数据淘汰，直到腾出足够空间为止</td>
        </tr>
        <tr>
            <td>allkeys-random</td>
            <td>从所有数据集中任意选择数据进行淘汰，直到腾出足够空间为止</td>
        </tr>
        <tr>
            <td>noeviction</td>
            <td>不淘汰数据，若超过最大内存，返回错误信息</td>
        </tr>
        <tr>
            <td colspan="2" style="font-size:13px;">Redis4.0后新增策略</td>
        </tr>
        <tr>
            <td>volatile-lfu</td>
            <td>从已设置过期时间的数据集中挑选最不经常使用(least frequently used)的数据进行淘汰，直到腾出足够的空间为止</td>
        </tr>
        <tr>
            <td>allkeys-lfu</td>
            <td>从所有数据集中挑选最不经常使用的数据进行淘汰，直到腾出足够的空间为止</td>
        </tr>
    </tbody>
</table>


Redis 的淘汰算法实际实现上并非针对所有 key，而是抽样一小部分并且从中选出被淘汰的 key。

使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。

### 10. 持久化

Redis 是内存型数据库，防止服务宕机内存数据丢失，需要把内存数据写到磁盘中去

Redis 有两种持久化方式 RDB(Redis DataBase)持久化、AOF(Append-only File)持久化，当两种方式**同时开启时**，数据恢复时Redis 会**优先选择 AOF 恢复**。

#### RDB 持久化

RDB 是 Redis 默认的持久化方式，当满足特定条件时，它将生成数据集的时间点快照，对应产生的数据文件为dump.rdb。可以通过配置文件中的 save 参数来定义产生快照的条件。save m n，指定当m秒内发生n次变化时，会触发bgsave

有两个Redis命令可以用于生成 RDB 文件，一个是SAVE（会阻塞 Redis 服务器进程，直到RDB文件创建完毕为止），另一个是 BGSAVE（派生出一个子进程，然后由子进程负责创建RDB文件，不会阻塞服务器进程）。

优点：

- 只有一个文件 dump.rdb，方便持久化。
- 容灾性好，一个文件可以保存到安全的磁盘。
- 性能最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 Redis 的高性能。
- 相对于数据集大时，比 AOF 的启动效率更高。

缺点：

- 数据安全性低。RDB 是间隔一段时间进行持久化的，如果系统发生故障，将会丢失最后一次创建快照之后的数据。

#### AOF 持久化

AOF 持久化，是将Redis执行的每次写命令添加到 AOF 文件的末尾。

AOF 持久化的同步选项：

<table style="text-align: center;width:700px">
    <tablehead>
        <tr>
            <th sytle="width:100px;">选项</th>
            <th style="width:200px;">同步频率</th>
            <th style="width:400px;">特点</th>
        </tr>
    </tablehead>
    <tablebody>
        <tr>
            <td>always</td>
            <td>每一个写命令都同步</td>
            <td>会严重降低服务器性能</td>
        </tr>
		<tr>
            <td>everysec</td>
            <td>每秒同步一次</td>
            <td>比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，对服务器性能几乎没有什么影响</td>
        </tr>
		<tr>
            <td>no</td>
            <td>让操作系统来决定何时同步</td>
            <td>并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量</td>
        </tr>
    </tablebody>
</table>

优点：

- 数据安全。AOF 可以记录每一次修改操作（使用 always 同步选项）
- 通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题

缺点：

- AOF 文件比 RDB 文件大，且恢复速度慢
- 数据集大的时候，启动效率比 RDB 低

> **Redis 4.0** 带来了一个新的持久化选项——**混合持久化**。将 `rdb` 文件的内容和增量的 AOF 日志文件存在一起。这里的 AOF 日志不再是全量的日志，而是 **两次 RDB 持久化之间** 的这段时间发生的增量 AOF 日志，通常这部分 AOF 日志很小。在 Redis 重启的时候，可以先加载 `rdb` 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。缺点是兼容性差，4.0之前不都不识别该持久化文件。

### 11. 事件

Redis服务器是一个事件驱动程序，服务器需要处理以下两类事件：

#### 文件事件

Redis服务器通过套接字与客户端（或者其他Redis服务器）进行连接，而文件事件就是服务器对套接字操作的抽象。

Redis基于 Reactor 模式开发了自己的网络事件处理器——文件事件处理器（file event handler），它使用 I/O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-file_event_handler.png" style="zoom:55%;" />

#### 时间事件

Redis服务器中的一些操作（比如serverCron函数）需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象。

Redis的时间事件分为以下两类：

- 定时事件：让一段程序在指定的时间之后执行一次。
- 周期性事件：让一段程序每隔指定时间就执行一次。

Redis 服务器将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，它就遍历整个链表，查找所有已到达的时间事件，并调用相应的事件处理器。

#### 事件的调度与执行

Redis 服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。

事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：

```python
def aeProcessEvents():
    # 获取到达时间离当前时间最接近的时间事件
    time_event = aeSearchNearestTimer()
    # 计算最接近的时间事件距离到达还有多少毫秒
    remaind_ms = time_event.when - unix_ts_now()
    # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0
    if remaind_ms < 0:
        remaind_ms = 0
    # 根据 remaind_ms 的值，创建 timeval
    timeval = create_timeval_with_ms(remaind_ms)
    # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定
    aeApiPoll(timeval)
    # 处理所有已产生的文件事件
    procesFileEvents()
    # 处理所有已到达的时间事件
    processTimeEvents()
```

将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下：

```python
def main():
    # 初始化服务器
    init_server()
    # 一直处理事件，直到服务器关闭为止
    while server_is_not_shutdown():
        aeProcessEvents()
    # 服务器关闭，执行清理操作
    clean_server()
```

从事件处理的角度来看，服务器运行流程如下：

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-enventProcess.png" style="zoom:50%;" />

### 12. 复制

#### 复制的过程

1. 从服务器执行`slaveof [masterIP] [masterPort]`，保存主服务器信息，此时从服务器并没有立即发起复制。

2. 从服务器中的定时任务发现主服务器信息，建立和主服务器的 socket 连接。

3. 从服务器向主服务器发送 PING 信号，如果从服务器读取到主服务器返回的 "PONG" 则执行下一个步骤，否则断开重连。

    >发送PING命令是为了检查套接字的读写状态是否正常；主服务器能否正常处理命令请求。
    >
    >从服务器读取到"PONG"回复，表示主从服务器之间的网络连接状态正常，并且主服务器可以正常处理从服务器发送的命令请求。

4. 如果主服务器设置了权限，那么就需要进行权限验证，成功则执行下一步，失败则复制终止，**需要从创建套接字开始重新执行复制**，直到身份验证通过，或者从服务器放弃执行复制为止。

5. 主服务器将所有数据发送给从服务器。（**数据同步**）

6. 主服务器把当前的数据同步给从服务器后，便完成了复制的建立过程。接下来，主服务器就会持续的把写命令发送给从服务器，保证主从数据一致性。（**命令传播**）

#### 数据同步

 Redis2.8 之前使用`sync`同步命令，Redis2.8 之后使用`psync`命令。两者不同在于，sync 命令仅支持完整重同步（全量复制），psync 支持完整重同步和部分重同步（部分复制）。

psync 的完整重同步用于处理初次复制的情况，部分重同步用于处理断线后重复制的情况。

sync 的执行步骤如下：

1. 从服务器向主服务器发送 SYNC 命令。
2. 收到 SYNC 命令的主服务器执行 BGSAVE 命令，在后台生成一个 RDB 文件，**并使用一个缓冲区记录从现在开始执行的所有写命令**。
3. 当主服务器的 BGSAVE 命令执行完毕时，主服务器将生成的 RDB 文件发送给从服务器，从服务器接收并载入这个 RDB 文件，将自己的数据库状态更新至主服务器执行 BGSAVE 命令时的数据库状态。
4. 主服务器将记录在缓冲区里面的所有写命令发送给从服务器，从服务器执行这些写命令，将自己的数据库状态更新至主服务器数据库当前所处的状态。

##### 完整重同步

psync 完整重同步的执行步骤和 SYNC 命令的执行步骤基本一样，都是通过让主服务器创建并发送RDB文件，以及向从服务器发送保存在缓冲区里面的写命令来进行同步。

##### 部分重同步

部分重同步功能的三个构成部分：

- replication offset，**复制偏移量**

    主服务器和从服务器都各自维护自己的主/从复制偏移量 offset，当主服务器有写入命令时，将自己的 offset **加上写入命令的字节长度**，从服务器每次收到主服务器传播来的N个字节的数据时，就将自己的 offset 加上N。

    通过对比主从服务器的复制偏移量，可以很容易地知道主从服务器是否处于一致状态

- replication backlog，**主服务器的复制积压缓冲区**

    复制积压缓冲区是由**主服务器维护的一个固定长度先进先出（FIFO）队列**，默认大小为1MB。

    当主服务器进行命令传播时，它不仅会将写命令发送给所有从服务器，还会将写命令入队到复制积压缓冲区里面。

    为了安全起见，可以将复制积压缓冲区的大小设为2$\times$断线重连需要的平均时间$\times$主服务器平均每秒产生的写命令数据量

- runID **服务器的运行ID**

    每个 redis 节点启动都会生成唯一的 uuid，每次 redis 重启后，runId 都会发生变化。

部分重同步的步骤：

1. 当从服务器断线并重新连上一个主服务器时，向主服务器发送`psync runID offset`命令
2. 主服务器收到命令后**首先检查参数 runID 是否与自身一致**，如果一致，说明之前复制的是当前主服务器，可以继续尝试执行部分重同步操作，否则执行完整重同步操作。**然后检查偏移量 offset 之后的数据**（也即是偏移量offset+1开始的数据）**是否存在于复制积压缓冲区里面**，存在则对从服务器执行部分重同步操作；不存在则对从服务器执行完整重同步操作。
3. 主服务器将复制积压缓冲区 offset 偏移量之后的所有数据都发送给从服务器。
4. 从服务器接收到缺失的数据，就可以回到与主服务器一致的状态。

##### PSYNC命令

接收到 PSYNC 命令的主服务器有三种响应

- **+FULLRESYNC \<runID> \<offset>** ：主服务器将对从服务器执行完整重同步操作。这里的 runID 和 offset 都是主服务器的，从服务器会将 runID 保存起来以便下次使用，并且会用 offset 作为自己的初始化偏移量。
- **+CONTINUE** ：主服务器将对从服务器执行部分重同步操作。
- **-ERR** ：主服务器版本低于2.8不支持 psync 命令，从服务器将向主服务器发送SYNC命令，并进行完整同步。

<div style="text-align:center;">
    <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-copy-progress_of_PSYNC.png" style="zoom:50%;" />
    <p><b>PSYNC 执行完整重同步和部分重同步时可能遇上的情况</b></p>
</div>

#### 心跳检测

在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务器发送命令：

```c
REPLCONF ACK <replication_offset>  // replication_offsets 是从服务器当前复制偏移量
```

心跳检测的作用：

- **检测主从服务器的网络连接状态**

    如果主服务器超过一秒钟没有收到从服务器发来的`REPLCONF ACK`命令，那么主服务器就知道主从服务器之间的连接出现问题了。

- **辅助实现 min-slave 选项**

- **检测命令丢失**

    如果因为网络故障，主服务器传播给从服务器的写命令在半路丢失，那么当从服务器向主服务器发送`REPLCONF ACK`命令时，主服务器将发觉从服务器当前的复制偏移量少于自己的复制偏移量，然后主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区里面找到从服务器缺少的数据，并将这些数据重新发送给从服务器。

### 13. 哨兵（sentinel）

#### Sentinel 架构

<img src="https://gitee.com/coldsun233/NotePic/raw/master/img/redis-sentinel.png" style="zoom:65%;" />

上图为 Redis sentinel 架构图，它由两部分组成，哨兵节点与数据节点：

- 哨兵节点：一个或多个哨兵节点组成哨兵系统，哨兵节点本质上是一种特殊的 Redis 服务器。
- 数据节点：主节点与从节点都是数据节点。

哨兵主要有以下功能：

- **监控（Monitoring）：** 哨兵会不断地检查主节点和从节点是否运作正常。
- **自动故障转移（Automatic failover）：** 当 主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。
- **通知（Notification）：** 哨兵可以将故障转移的结果发送给客户端。
- **提供配置信息（Configuration provider）：** **客户端**在初始化时，**通过连接哨兵来获得当前 Redis 服务的主节点地址**。

#### 检测主观下线状态和客观下线状态

1. 在默认情况下，Sentinel会以每秒一次的频率向所有与它创建连接的主服务器，从服务器以及其他的 Sentinel 实例发 PING 命令。

2. 如果一个实例在配置的`down-after-milliseconds`毫秒内没有有效的回复 PING 命令，那么这个实例会被 Sentinel 标记为**主观下线**。

    > **注意**：每一个 Sentinel 所设置的`down-after-milliseconds`选项的值也可能不同，因此，当一个 Sentinel 将实例判断为主观下线时，其他 Sentinel 可能仍然会认为该实例处于在线状态。

3. 当 Sentinel 将一个实例判断为主观下线之后，为了确认这个主服务器是否真的下线了，它会向同样监视这一实例的其他Sentinel 进行询问，看它们是否也认为该实例已经进入了主观下线状态。

4. 当认为实例已经进入下线状态的 Sentinel 的数量，超过 Sentinel 配置中设置的`quorum`参数的值，那么 Sentinel 就会认为实例已经进入**客观下线状态**。(不同 Sentinel 判断客观下线的条件可能不同)

#### 故障转移

当主服务器的下线时长超过用户设定的下线时长上限时，Sentinel 系统就会对主服务器执行故障转移操作：

1. 在已下线主服务器属下的所有从服务器里面，挑选出一个从服务器，并将其转换为主服务器。
2. 让已下线主服务器属下的所有从服务器改为复制新的主服务器。
3. 继续监视已下线的主服务器，并在它重新上线时，将它设置为新的主服务器的从服务器。

**新的主服务器的挑选**

领头 Sentinel 会将已下线主服务器的所有从服务器保存到一个列表里面，然后按照以下规则，一项一项地对列表进行过滤：

- 删除列表中所有处于下线或者断线状态的从服务器，这保证列表中剩余的从服务器都是正常在线的。
- 删除列表中所有最近五秒内没有回复过领头 Sentinel 的 INFO 命令的从服务器，这保证列表中剩余的从服务器都是最近成功进行过通信的。
- 删除所有与已下线主服务器连接断开超过`down-after-milliseconds*10`毫秒的从服务器，这保证列表中剩余的从服务器保存的数据都是比较新的。

然后在剩下的从服务器中挑选优先级最高的作为新的主服务器；如果多个相同最高优先级的从服务器，则选择复制偏移量最大的；如果有多个优先级最高、复制偏移量最大的从服务器，则选择运行 ID 最小的。

### 14. 集群

#### 数据分片

集群将数据分散到多个节点，**一方面** 突破了 Redis 单机内存大小的限制，**存储容量大大增加**；**另一方面** 每个主节点都可以对外提供读服务和写服务，**大大提高了集群的响应能力**。

集群的整个数据库被分为16384个槽（slot），在 Redis 中节点通过计算每个 key 的 `CRC16 校验和`对16384取模来决定 key 属于哪个槽，集群中的每个节点可以处理0个或最多16384个槽。

**分片实现方案**：

- **客户端分片**：客户端使用一致性哈希等算法决定键应当分布到哪个节点。
- **代理分片**：将客户端请求发送到代理上，由代理转发请求到正确的节点上。
- **服务器分片**：客户端直接向服务器中任意结点请求，如果节点发现键所在的槽并非由自己负责处理就会向客户端返回一个 MOVED 错误，当客户端接收到节点返回的 MOVED 错误时，客户端会根据MOVED错误中提供的 IP 地址和端口号，转向至负责处理槽的节点，并向该节点重新发送之前想要执行的命令。

#### 故障转移

Redis 的节点分为主节点和节点，主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。

使用命令`CLUSTER REPLICATE <node_id>`可以让接收命令的节点成为node_id所指定节点的从节点，并开始对主节点进行复制。

当一个从节点发现自己正在复制的主节点进入了已下线状态时，从节点将开始对下线主节点进行故障转移，以下是故障转移的执行步骤：

- 复制下线主节点的所有从节点里面，会有一个从节点被选中。
- 被选中的从节点会执行 SLAVEOF no one 命令，成为新的主节点。
- 新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。
- 新的主节点向集群广播一条 PONG 消息，这条 PONG 消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽。
- 新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。

### 15. 事务

Redis 事务功能是通过 MULTI、EXEC、DISCARD 和 WATCH 四个原语实现的。事务能够一次性、按顺序地执行多个命令，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，事务中的所有命令都执行完毕，才会去处理其他客户端的命令请求。

#### 事务的三个阶段

1. **事务开始** MULTI

    MULTI 总是返回OK，将执行该命令的客户端从非事务状态切换至事务状态。

2. **命令入队**

    客户端切换到事务状态之后，服务器如果收到MULTI、EXEC、DISCARD、WATCH四个命令的其中一个，它会立即执行这个命令，对于四个命令以外的其他命令，服务器不会立即执行这个命令，而是将这个命令放入一个事务队列里面，然后向客户端返回QUEUED回复。

3. **事务执行** EXEC

    服务器会遍历这个客户端的事务队列，执行队列中保存的所有命令，最后将执行命令所得的结果全部返回给客户端。当操作被打断时，返回空值 nil。

**通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出**。

#### WATCH 命令

WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为，它可以**在 EXEC 命令执行之前,监视任意数量的键**，一旦其中有一个键被修改或删除，服务器会拒绝事务的执行，并向客户端返回空值 nil。

UNWATCH 命令可以取消 WATCH 对**所有键**的监控。

**实现原理**

- 每个Redis数据库都保存着一个 watched_keys 字典，这个字典的 **key 是某个被 WATCH 命令监视的数据库键**，而字典的 **value 则是一个链表，链表中记录了所有监视相应数据库键的客户端**。

    <div style="display: flex;">
        <div style="flex:1;">
            例如右图表示:<br/>
            &nbsp;&nbsp;&nbsp;&nbsp;❑客户端c1和c2正在监视键"name"。<br/>
            &nbsp;&nbsp;&nbsp;&nbsp;❑客户端c3正在监视键"age"。<br/>
            &nbsp;&nbsp;&nbsp;&nbsp;❑客户端c2和c4正在监视键"address"。
        </div>
        <div style="flex:1;">
            <img src="https://gitee.com/coldsun233/NotePic/raw/master/img/watched_key.jpg" style="zoom:60%;" />
        </div>
    </div>

-  所有对数据库进行修改的命令，比如SET、LPUSH、SADD、ZREM、DEL、FLUSHDB等等，在执行之后都会对watched_keys 字典进行检查，查看是否有客户端正在监视刚刚被命令修改过的数据库键，如果有的话，那么会将监视被修改键的客户端的 REDIS_DIRTY_CAS 标识打开，表示该客户端的事务安全性已经被破坏。

- 当服务器接收到一个客户端发来的EXEC命令时，如果服务器检测到客户端的 REDIS_DIRTY_CAS 标识已经被打开，它会服务器会拒绝执行客户端提交的事务。

#### 事务的 ACID 性质

在Redis中，事务**总是具有原子性**（Atomicity）、**一致性**（Consistency）和**隔离性**（Isolation），**只有当 Redis 服务器运行在 AOF 持久化模式下，并且 appendfsync 选项的值为 always 时，事务将具有耐久性**（Durability）。

### tail